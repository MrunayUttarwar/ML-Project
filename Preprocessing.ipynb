{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBtrHUQyi2zj",
        "outputId": "4ed05eb9-7a07-4ce8-bcf0-744b3fc6f35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF results saved as 'TFIDF_Output.csv'.\n",
            "Preprocessing complete. Cleaned data saved as 'Cleaned_ML.csv'.\n",
            "Sentiment impact by Device_Type:\n",
            " Device_Type\n",
            "Desktop   -0.017236\n",
            "Mobile     0.021005\n",
            "Tablet    -0.009977\n",
            "Name: Sentiment_Encoded, dtype: float64\n",
            "\n",
            "Sentiment impact by Location:\n",
            " Location\n",
            "Berlin, Germany            0.008114\n",
            "Cape Town, South Africa    0.028112\n",
            "London, UK                -0.074427\n",
            "Mumbai, India              0.005181\n",
            "New York, USA             -0.009045\n",
            "Paris, France              0.047813\n",
            "Sao Paulo, Brazil         -0.014521\n",
            "Sydney, Australia          0.055165\n",
            "Tokyo, Japan              -0.073602\n",
            "Toronto, Canada            0.012270\n",
            "Name: Sentiment_Encoded, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from contractions import fix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load dataset (Replace with your actual path if using local file)\n",
        "df = pd.read_excel(\"ML.xlsx\")\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Expand contractions (e.g., \"can't\" â†’ \"cannot\")\n",
        "    text = fix(text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove special characters & punctuation (excluding emojis)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join words back into a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply cleaning function to 'Post_Text' column\n",
        "df['Cleaned_Post_Text'] = df['Post_Text'].apply(clean_text)\n",
        "\n",
        "# Remove '#' from Hashtags column and extract hashtags separately\n",
        "df['Hashtags'] = df['Hashtags'].str.replace('#', '')\n",
        "\n",
        "# Convert emojis into their text meaning\n",
        "df['Post_Text_Emojis_Converted'] = df['Post_Text'].apply(lambda x: emoji.demojize(x, delimiters=(\" \", \" \")))\n",
        "\n",
        "# Encode Sentiments\n",
        "df['Sentiment_Encoded'] = df['Sentiment'].map({'Positive': 1, 'Negative': -1, 'Neutral': 0})\n",
        "\n",
        "# Feature Engineering: Word Count and Emoji Count\n",
        "df['Word_Count'] = df['Cleaned_Post_Text'].apply(lambda x: len(x.split()))\n",
        "df['Emoji_Count'] = df['Post_Text'].apply(lambda x: sum(1 for char in x if char in emoji.EMOJI_DATA))\n",
        "\n",
        "# Analyze Device_Type and Location impact on Sentiment\n",
        "device_sentiment = df.groupby('Device_Type')['Sentiment_Encoded'].mean()\n",
        "location_sentiment = df.groupby('Location')['Sentiment_Encoded'].mean()\n",
        "\n",
        "# Tokenization & Vectorization\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "df_tfidf = tfidf_vectorizer.fit_transform(df['Cleaned_Post_Text']).toarray()\n",
        "\n",
        "# Get feature names (words corresponding to TF-IDF values)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert TF-IDF matrix into a DataFrame\n",
        "df_tfidf_result = pd.DataFrame(df_tfidf, columns=feature_names)\n",
        "\n",
        "# Add original post IDs for reference (if available)\n",
        "df_tfidf_result.insert(0, 'Post_ID', df.index)\n",
        "\n",
        "\n",
        "df_tfidf_result.to_csv(\"TFIDF_Output.csv\", index=False)\n",
        "print(\"TF-IDF results saved as 'TFIDF_Output.csv'.\")\n",
        "\n",
        "df.to_csv(\"Cleaned_ML.csv\", index=False)\n",
        "print(\"Preprocessing complete. Cleaned data saved as 'Cleaned_ML.csv'.\")\n",
        "\n",
        "# Display impact analysis results\n",
        "print(\"Sentiment impact by Device_Type:\\n\", device_sentiment)\n",
        "print(\"\\nSentiment impact by Location:\\n\", location_sentiment)\n"
      ]
    }
  ]
}